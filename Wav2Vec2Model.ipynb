{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a218b67c-1325-4897-972b-40948c9a0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from fairseq.dataclass import ChoiceEnum, FairseqDataclass\n",
    "from fairseq.modules import (\n",
    "    Fp32GroupNorm,\n",
    "    Fp32LayerNorm,\n",
    "    GradMultiply,\n",
    "    GumbelVectorQuantizer,\n",
    "    LayerNorm,\n",
    "    MultiheadAttention,\n",
    "    SamePad,\n",
    "    TransposeLast,\n",
    ")\n",
    "\n",
    "from fairseq.models import BaseFairseqModel, register_model\n",
    "from fairseq.modules.transformer_sentence_encoder import init_bert_params\n",
    "from fairseq.data.data_utils import compute_mask_indices\n",
    "from fairseq.utils import buffered_arange, index_put, is_xla_tensor\n",
    "\n",
    "from ConvFeatureExtractionModel import ConvFeatureExtractionModelConfig, ConvFeatureExtractionModel\n",
    "from TransformerSentenceEncoderLayer import TransformerSentenceEncoderLayerConfig, TransformerSentenceEncoderLayer\n",
    "from TransformerEncoder import TransformerEncoderConfig, TransformerEncoder\n",
    "\n",
    "MASKING_DISTRIBUTION_CHOICES = ChoiceEnum([\"static\", \"uniform\", \"normal\", \"poisson\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35cc3da-f4cf-4e94-b167-122069f727b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Wav2Vec2Config(FairseqDataclass):\n",
    "    \n",
    "    conv_layer_setting: ConvFeatureExtractionModelConfig = field(\n",
    "        default=ConvFeatureExtractionModelConfig(),\n",
    "        metadata={\"help\": \"Default setting of ConvFeatureExtractionModelConfig\"}\n",
    "    )\n",
    "    \n",
    "    encoder_setting: TransformerEncoderConfig = field(\n",
    "        default=TransformerEncoderConfig(),\n",
    "        metadata={\"help\": \"Default setting of TransformerEncoderConfig\"}\n",
    "    )\n",
    "    \n",
    "    dropout_input: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"dropout to apply to the input (after feat extr)\"},\n",
    "    )\n",
    "    dropout_features: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"dropout to apply to the features (after feat extr)\"},\n",
    "    )\n",
    "\n",
    "    final_dim: int = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"project final representations and targets to this many dimensions.\"\n",
    "            \"set to encoder_embed_dim is <= 0\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    logit_temp: float = field(\n",
    "        default=0.1, metadata={\"help\": \"temperature to divide logits by\"}\n",
    "    )\n",
    "    quantize_targets: bool = field(\n",
    "        default=False, metadata={\"help\": \"use quantized targets\"}\n",
    "    )\n",
    "    quantize_input: bool = field(\n",
    "        default=False, metadata={\"help\": \"use quantized inputs\"}\n",
    "    )\n",
    "    same_quantizer: bool = field(\n",
    "        default=False, metadata={\"help\": \"use same quantizer for inputs and targets\"}\n",
    "    )\n",
    "    target_glu: bool = field(\n",
    "        default=False, metadata={\"help\": \"adds projection + glu to targets\"}\n",
    "    )\n",
    "    feature_grad_mult: float = field(\n",
    "        default=1.0, metadata={\"help\": \"multiply feature extractor var grads by this\"}\n",
    "    )\n",
    "    quantizer_depth: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"number of quantizer layers\"},\n",
    "    )\n",
    "    quantizer_factor: int = field(\n",
    "        default=3,\n",
    "        metadata={\n",
    "            \"help\": \"dimensionality increase for inner quantizer layers (if depth > 1)\"\n",
    "        },\n",
    "    )\n",
    "    latent_vars: int = field(\n",
    "        default=320,\n",
    "        metadata={\"help\": \"number of latent variables V in each group of the codebook\"},\n",
    "    )\n",
    "    latent_groups: int = field(\n",
    "        default=2,\n",
    "        metadata={\"help\": \"number of groups G of latent variables in the codebook\"},\n",
    "    )\n",
    "    latent_dim: int = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"if > 0, uses this dimensionality for latent variables. \"\n",
    "            \"otherwise uses final_dim / latent_groups\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # masking\n",
    "    mask_length: int = field(default=10, metadata={\"help\": \"mask length\"})\n",
    "    mask_prob: float = field(\n",
    "        default=0.65, metadata={\"help\": \"probability of replacing a token with mask\"}\n",
    "    )\n",
    "    mask_selection: MASKING_DISTRIBUTION_CHOICES = field(\n",
    "        default=\"static\", metadata={\"help\": \"how to choose mask length\"}\n",
    "    )\n",
    "    mask_other: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"secondary mask argument (used for more complex distributions), \"\n",
    "            \"see help in compute_mask_indices\"\n",
    "        },\n",
    "    )\n",
    "    no_mask_overlap: bool = field(\n",
    "        default=False, metadata={\"help\": \"whether to allow masks to overlap\"}\n",
    "    )\n",
    "    mask_min_space: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"min space between spans (if no overlap is enabled)\"},\n",
    "    )\n",
    "\n",
    "    # channel masking\n",
    "    mask_channel_length: int = field(\n",
    "        default=10, metadata={\"help\": \"length of the mask for features (channels)\"}\n",
    "    )\n",
    "    mask_channel_prob: float = field(\n",
    "        default=0.0, metadata={\"help\": \"probability of replacing a feature with 0\"}\n",
    "    )\n",
    "    mask_channel_before: bool = False\n",
    "    mask_channel_selection: MASKING_DISTRIBUTION_CHOICES = field(\n",
    "        default=\"static\",\n",
    "        metadata={\"help\": \"how to choose mask length for channel masking\"},\n",
    "    )\n",
    "    mask_channel_other: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"secondary mask argument (used for more complex distributions), \"\n",
    "            \"see help in compute_mask_indicesh\"\n",
    "        },\n",
    "    )\n",
    "    no_mask_channel_overlap: bool = field(\n",
    "        default=False, metadata={\"help\": \"whether to allow channel masks to overlap\"}\n",
    "    )\n",
    "    mask_channel_min_space: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"min space between spans (if no overlap is enabled)\"},\n",
    "    )\n",
    "\n",
    "    # negative selection\n",
    "    num_negatives: int = field(\n",
    "        default=100,\n",
    "        metadata={\"help\": \"number of negative examples from the same sample\"},\n",
    "    )\n",
    "    negatives_from_everywhere: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"sample negatives from everywhere, not just masked states\"},\n",
    "    )\n",
    "    cross_sample_negatives: int = field(\n",
    "        default=0, metadata={\"help\": \"number of negative examples from the any sample\"}\n",
    "    )\n",
    "    codebook_negatives: int = field(\n",
    "        default=0, metadata={\"help\": \"number of negative examples codebook\"}\n",
    "    )\n",
    "\n",
    "    # positional embeddings\n",
    "    latent_temp: Tuple[float, float, float] = field(\n",
    "        default=(2, 0.5, 0.999995),\n",
    "        metadata={\n",
    "            \"help\": \"temperature for latent variable sampling. \"\n",
    "            \"can be tuple of 3 values (start, end, decay)\"\n",
    "        },     \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9996e9f-a3a3-473c-8c63-1e48f65372bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@register_model(\"wav2vec2\", dataclass=Wav2Vec2Config)\n",
    "class Wav2Vec2Model(BaseFairseqModel):\n",
    "    def __init__(self, cfg: Wav2Vec2Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Feature extraction module\n",
    "        feature_extractor_cfg = cfg.conv_layer_setting\n",
    "        self.feature_extractor = ConvFeatureExtractionModel(feature_extractor_cfg)\n",
    "                \n",
    "        # FC between Feature extractor & Transformer encoder layer module\n",
    "        feature_enc_layers = eval(feature_extractor_cfg.conv_feature_layers)\n",
    "        self.feature_extractor_output_embed = feature_enc_layers[-1][0]\n",
    "\n",
    "        transformer_encoder_cfg = cfg.encoder_setting\n",
    "        transformer_encoder_layer_cfg = transformer_encoder_cfg.layer_setting\n",
    "        \n",
    "        self.post_extract_proj = (\n",
    "            nn.Linear(self.feature_extractor_output_embed,\n",
    "                      transformer_encoder_layer_cfg.encoder_embed_dim)\n",
    "            if (self.feature_extractor_output_embed !=\n",
    "                transformer_encoder_layer_cfg.encoder_embed_dim\n",
    "                and not cfg.quantize_input)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.mask_prob = cfg.mask_prob\n",
    "        self.mask_selection = cfg.mask_selection\n",
    "        self.mask_other = cfg.mask_other\n",
    "        self.mask_length = cfg.mask_length\n",
    "        self.no_mask_overlap = cfg.no_mask_overlap\n",
    "        self.mask_min_space = cfg.mask_min_space\n",
    "\n",
    "        self.mask_channel_prob = cfg.mask_channel_prob\n",
    "        self.mask_channel_before = cfg.mask_channel_before\n",
    "        self.mask_channel_selection = cfg.mask_channel_selection\n",
    "        self.mask_channel_other = cfg.mask_channel_other\n",
    "        self.mask_channel_length = cfg.mask_channel_length\n",
    "        self.no_mask_channel_overlap = cfg.no_mask_channel_overlap\n",
    "        self.mask_channel_min_space = cfg.mask_channel_min_space\n",
    "\n",
    "        self.dropout_input = nn.Dropout(cfg.dropout_input)\n",
    "        self.dropout_features = nn.Dropout(cfg.dropout_features)\n",
    "\n",
    "        self.feature_grad_mult = cfg.feature_grad_mult\n",
    "\n",
    "        self.quantizer = None\n",
    "        self.input_quantizer = None\n",
    "\n",
    "        self.n_negatives = cfg.num_negatives\n",
    "        self.cross_sample_negatives = cfg.cross_sample_negatives\n",
    "        self.codebook_negatives = cfg.codebook_negatives\n",
    "        self.negatives_from_everywhere = cfg.negatives_from_everywhere\n",
    "\n",
    "        self.logit_temp = cfg.logit_temp\n",
    "\n",
    "        final_dim = (cfg.final_dim\n",
    "        if cfg.final_dim > 0\n",
    "        else transformer_encoder_layer_cfg.encoder_embed_dim)\n",
    "\n",
    "        if cfg.quantize_targets:\n",
    "            vq_dim = (cfg.latent_dim\n",
    "            if cfg.latent_dim > 0\n",
    "            else final_dim)\n",
    "            self.quantizer = GumbelVectorQuantizer(\n",
    "                dim=self.feature_extractor_output_embed,\n",
    "                num_vars=cfg.latent_vars,\n",
    "                temp=cfg.latent_temp,\n",
    "                groups=cfg.latent_groups,\n",
    "                combine_groups=False,\n",
    "                vq_dim=vq_dim,\n",
    "                time_first=True,\n",
    "                weight_proj_depth=cfg.quantizer_depth,\n",
    "                weight_proj_factor=cfg.quantizer_factor,\n",
    "                )\n",
    "            self.project_q = nn.Linear(vq_dim, final_dim)\n",
    "        else:\n",
    "            self.project_q = nn.Linear(self.feature_extractor_output_embed, final_dim)\n",
    "\n",
    "        if cfg.quantize_input:\n",
    "            if cfg.same_quantizer and self.quantizer is not None:\n",
    "                vq_dim = final_dim\n",
    "                self.input_quantizer = self.quantizer\n",
    "            else:\n",
    "                vq_dim = (cfg.latent_dim\n",
    "                if cfg.latent_dim > 0\n",
    "                else transformer_encoder_layer_cfg.encoder_embed_dim)\n",
    "                self.input_quantizer = GumbelVectorQuantizer(\n",
    "                    dim=self.feature_extractor_output_embed,\n",
    "                    num_vars=cfg.latent_vars,\n",
    "                    temp=cfg.latent_temp,\n",
    "                    groups=cfg.latent_groups,\n",
    "                    combine_groups=False,\n",
    "                    vq_dim=vq_dim,\n",
    "                    time_first=True,\n",
    "                    weight_proj_depth=cfg.quantizer_depth,\n",
    "                    weight_proj_factor=cfg.quantizer_factor,\n",
    "                    )\n",
    "            self.project_inp = nn.Linear(vq_dim,\n",
    "                                         transformer_encoder_layer_cfg.encoder_embed_dim)\n",
    "\n",
    "        self.mask_emb = nn.Parameter(\n",
    "            torch.FloatTensor(transformer_encoder_layer_cfg.encoder_embed_dim).uniform_()\n",
    "            )\n",
    "\n",
    "        self.encoder = TransformerEncoder(transformer_encoder_cfg)\n",
    "        self.layer_norm = LayerNorm(self.feature_extractor_output_embed)\n",
    "\n",
    "        self.target_glu = None\n",
    "        if cfg.target_glu:\n",
    "            self.target_glu = nn.Sequential(\n",
    "                nn.Linear(final_dim, final_dim * 2), nn.GLU()\n",
    "                )\n",
    "\n",
    "        self.final_proj = nn.Linear(transformer_encoder_layer_cfg.encoder_embed_dim,\n",
    "                                    final_dim)\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        super().upgrade_state_dict_named(state_dict, name)\n",
    "        \"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"\n",
    "        return state_dict\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, cfg: Wav2Vec2Config, task=None):\n",
    "        \"\"\"Build a new model instance.\"\"\"\n",
    "\n",
    "        return cls(cfg)\n",
    "\n",
    "    def apply_mask(\n",
    "        self,\n",
    "        x,\n",
    "        padding_mask,\n",
    "        mask_indices=None,\n",
    "        mask_channel_indices=None,\n",
    "    ):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        if self.mask_channel_prob > 0 and self.mask_channel_before:\n",
    "            mask_channel_indices = compute_mask_indices(\n",
    "                (B, C),\n",
    "                None,\n",
    "                self.mask_channel_prob,\n",
    "                self.mask_channel_length,\n",
    "                self.mask_channel_selection,\n",
    "                self.mask_channel_other,\n",
    "                no_overlap=self.no_mask_channel_overlap,\n",
    "                min_space=self.mask_channel_min_space,\n",
    "            )\n",
    "            mask_channel_indices = (\n",
    "                torch.from_numpy(mask_channel_indices)\n",
    "                .to(x.device)\n",
    "                .unsqueeze(1)\n",
    "                .expand(-1, T, -1)\n",
    "            )\n",
    "            x[mask_channel_indices] = 0\n",
    "\n",
    "        if self.mask_prob > 0:\n",
    "            if mask_indices is None:\n",
    "                mask_indices = compute_mask_indices(\n",
    "                    (B, T),\n",
    "                    padding_mask,\n",
    "                    self.mask_prob,\n",
    "                    self.mask_length,\n",
    "                    self.mask_selection,\n",
    "                    self.mask_other,\n",
    "                    min_masks=2,\n",
    "                    no_overlap=self.no_mask_overlap,\n",
    "                    min_space=self.mask_min_space,\n",
    "                )\n",
    "                mask_indices = torch.from_numpy(mask_indices).to(x.device)\n",
    "            x = index_put(x, mask_indices, self.mask_emb)\n",
    "        else:\n",
    "            mask_indices = None\n",
    "\n",
    "        if self.mask_channel_prob > 0 and not self.mask_channel_before:\n",
    "            if mask_channel_indices is None:\n",
    "                mask_channel_indices = compute_mask_indices(\n",
    "                    (B, C),\n",
    "                    None,\n",
    "                    self.mask_channel_prob,\n",
    "                    self.mask_channel_length,\n",
    "                    self.mask_channel_selection,\n",
    "                    self.mask_channel_other,\n",
    "                    no_overlap=self.no_mask_channel_overlap,\n",
    "                    min_space=self.mask_channel_min_space,\n",
    "                )\n",
    "                mask_channel_indices = (\n",
    "                    torch.from_numpy(mask_channel_indices)\n",
    "                    .to(x.device)\n",
    "                    .unsqueeze(1)\n",
    "                    .expand(-1, T, -1)\n",
    "                )\n",
    "            x = index_put(x, mask_channel_indices, 0)\n",
    "\n",
    "        return x, mask_indices\n",
    "\n",
    "    def sample_negatives(self, y, num, padding_count=None):\n",
    "\n",
    "        if self.n_negatives == 0 and self.cross_sample_negatives == 0:\n",
    "            return y.new(0)\n",
    "\n",
    "        bsz, tsz, fsz = y.shape\n",
    "        y = y.view(-1, fsz)  # BTC => (BxT)C\n",
    "\n",
    "        # FIXME: what happens if padding_count is specified?\n",
    "        cross_high = tsz * bsz\n",
    "        high = tsz - (padding_count or 0)\n",
    "        with torch.no_grad():\n",
    "            assert high > 1, f\"{bsz,tsz,fsz}\"\n",
    "\n",
    "            if self.n_negatives > 0:\n",
    "                tszs = (\n",
    "                    buffered_arange(num)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand(-1, self.n_negatives)\n",
    "                    .flatten()\n",
    "                )\n",
    "\n",
    "                neg_idxs = torch.randint(\n",
    "                    low=0, high=high - 1, size=(bsz, self.n_negatives * num)\n",
    "                )\n",
    "                neg_idxs[neg_idxs >= tszs] += 1\n",
    "\n",
    "            if self.cross_sample_negatives > 0:\n",
    "                tszs = (\n",
    "                    buffered_arange(num)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand(-1, self.cross_sample_negatives)\n",
    "                    .flatten()\n",
    "                )\n",
    "\n",
    "                cross_neg_idxs = torch.randint(\n",
    "                    low=0,\n",
    "                    high=cross_high - 1,\n",
    "                    size=(bsz, self.cross_sample_negatives * num),\n",
    "                )\n",
    "                cross_neg_idxs[cross_neg_idxs >= tszs] += 1\n",
    "\n",
    "        if self.n_negatives > 0:\n",
    "            for i in range(1, bsz):\n",
    "                neg_idxs[i] += i * high\n",
    "        else:\n",
    "            neg_idxs = cross_neg_idxs\n",
    "\n",
    "        if self.cross_sample_negatives > 0 and self.n_negatives > 0:\n",
    "            neg_idxs = torch.cat([neg_idxs, cross_neg_idxs], dim=1)\n",
    "\n",
    "        negs = y[neg_idxs.view(-1)]\n",
    "        negs = negs.view(\n",
    "            bsz, num, self.n_negatives + self.cross_sample_negatives, fsz\n",
    "        ).permute(\n",
    "            2, 0, 1, 3\n",
    "        )  # to NxBxTxC\n",
    "        return negs, neg_idxs\n",
    "\n",
    "    def compute_preds(self, x, y, negatives):\n",
    "\n",
    "        neg_is_pos = (y == negatives).all(-1)\n",
    "        y = y.unsqueeze(0)\n",
    "        targets = torch.cat([y, negatives], dim=0)\n",
    "\n",
    "        logits = torch.cosine_similarity(x.float(), targets.float(), dim=-1).type_as(x)\n",
    "\n",
    "        logits = logits / self.logit_temp\n",
    "\n",
    "        if is_xla_tensor(logits) or neg_is_pos.any():\n",
    "            fillval = -float(2 ** 30)\n",
    "            if not hasattr(self, \"_inftensor\"):\n",
    "                self._inftensor = (\n",
    "                    torch.tensor(fillval).to(x.device)\n",
    "                    if is_xla_tensor(logits)\n",
    "                    else float(\"-inf\")\n",
    "                )\n",
    "            logits[1:] = index_put(logits[1:], neg_is_pos, self._inftensor)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Computes the output length of the convolutional layers\n",
    "        \"\"\"\n",
    "\n",
    "        def _conv_out_length(input_length, kernel_size, stride):\n",
    "            return torch.floor((input_length - kernel_size) / stride + 1)\n",
    "\n",
    "        conv_cfg_list = eval(self.cfg.conv_feature_layers)\n",
    "\n",
    "        for i in range(len(conv_cfg_list)):\n",
    "            input_lengths = _conv_out_length(\n",
    "                input_lengths, conv_cfg_list[i][1], conv_cfg_list[i][2]\n",
    "            )\n",
    "\n",
    "        return input_lengths.to(torch.long)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source,\n",
    "        padding_mask=None,\n",
    "        mask=True,\n",
    "        features_only=False,\n",
    "        layer=None,\n",
    "        mask_indices=None,\n",
    "        mask_channel_indices=None,\n",
    "        padding_count=None,\n",
    "    ):\n",
    "\n",
    "        if self.feature_grad_mult > 0:\n",
    "            features = self.feature_extractor(source)\n",
    "            if self.feature_grad_mult != 1.0:\n",
    "                features = GradMultiply.apply(features, self.feature_grad_mult)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                features = self.feature_extractor(source)\n",
    "\n",
    "        features_pen = features.float().pow(2).mean()\n",
    "\n",
    "        features = features.transpose(1, 2)\n",
    "        features = self.layer_norm(features)\n",
    "        unmasked_features = features.clone()\n",
    "\n",
    "        if padding_mask is not None and padding_mask.any():\n",
    "            input_lengths = (1 - padding_mask.long()).sum(-1)\n",
    "            # apply conv formula to get real output_lengths\n",
    "            output_lengths = self._get_feat_extract_output_lengths(input_lengths)\n",
    "\n",
    "            padding_mask = torch.zeros(\n",
    "                features.shape[:2], dtype=features.dtype, device=features.device\n",
    "            )\n",
    "\n",
    "            # these two operations makes sure that all values\n",
    "            # before the output lengths indices are attended to\n",
    "            padding_mask[\n",
    "                (\n",
    "                    torch.arange(padding_mask.shape[0], device=padding_mask.device),\n",
    "                    output_lengths - 1,\n",
    "                )\n",
    "            ] = 1\n",
    "            padding_mask = (1 - padding_mask.flip([-1]).cumsum(-1).flip([-1])).bool()\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        if self.post_extract_proj is not None:\n",
    "            features = self.post_extract_proj(features)\n",
    "\n",
    "        features = self.dropout_input(features)\n",
    "        unmasked_features = self.dropout_features(unmasked_features)\n",
    "\n",
    "        num_vars = None\n",
    "        code_ppl = None\n",
    "        prob_ppl = None\n",
    "        curr_temp = None\n",
    "\n",
    "        if self.input_quantizer:\n",
    "            q = self.input_quantizer(features, produce_targets=False)\n",
    "            features = q[\"x\"]\n",
    "            num_vars = q[\"num_vars\"]\n",
    "            code_ppl = q[\"code_perplexity\"]\n",
    "            prob_ppl = q[\"prob_perplexity\"]\n",
    "            curr_temp = q[\"temp\"]\n",
    "            features = self.project_inp(features)\n",
    "\n",
    "        if mask:\n",
    "            x, mask_indices = self.apply_mask(\n",
    "                features,\n",
    "                padding_mask,\n",
    "                mask_indices=mask_indices,\n",
    "                mask_channel_indices=mask_channel_indices,\n",
    "            )\n",
    "            if not is_xla_tensor(x) and mask_indices is not None:\n",
    "                # tpu-comment: reducing the size in a dynamic way causes\n",
    "                # too many recompilations on xla.\n",
    "                y = unmasked_features[mask_indices].view(\n",
    "                    unmasked_features.size(0), -1, unmasked_features.size(-1)\n",
    "                )\n",
    "            else:\n",
    "                y = unmasked_features\n",
    "        else:\n",
    "            x = features\n",
    "            y = unmasked_features\n",
    "            mask_indices = None\n",
    "\n",
    "        x, layer_results = self.encoder(x, padding_mask=padding_mask, layer=layer)\n",
    "\n",
    "        if features_only:\n",
    "            return {\n",
    "                \"x\": x,\n",
    "                \"padding_mask\": padding_mask,\n",
    "                \"features\": unmasked_features,\n",
    "                \"layer_results\": layer_results,\n",
    "            }\n",
    "\n",
    "        if self.quantizer:\n",
    "            q = self.quantizer(y, produce_targets=False)\n",
    "            y = q[\"x\"]\n",
    "            num_vars = q[\"num_vars\"]\n",
    "            code_ppl = q[\"code_perplexity\"]\n",
    "            prob_ppl = q[\"prob_perplexity\"]\n",
    "            curr_temp = q[\"temp\"]\n",
    "\n",
    "            y = self.project_q(y)\n",
    "\n",
    "            if self.negatives_from_everywhere:\n",
    "                neg_cands = self.quantizer(unmasked_features, produce_targets=False)[\n",
    "                    \"x\"\n",
    "                ]\n",
    "                negs, _ = self.sample_negatives(\n",
    "                    neg_cands,\n",
    "                    y.size(1),\n",
    "                    padding_count=padding_count,\n",
    "                )\n",
    "                negs = self.project_q(negs)\n",
    "\n",
    "            else:\n",
    "                negs, _ = self.sample_negatives(\n",
    "                    y,\n",
    "                    y.size(1),\n",
    "                    padding_count=padding_count,\n",
    "                )\n",
    "\n",
    "            if self.codebook_negatives > 0:\n",
    "                cb_negs = self.quantizer.sample_from_codebook(\n",
    "                    y.size(0) * y.size(1), self.codebook_negatives\n",
    "                )\n",
    "                cb_negs = cb_negs.view(\n",
    "                    self.codebook_negatives, y.size(0), y.size(1), -1\n",
    "                )  # order doesnt matter\n",
    "                cb_negs = self.project_q(cb_negs)\n",
    "                negs = torch.cat([negs, cb_negs], dim=0)\n",
    "        else:\n",
    "            y = self.project_q(y)\n",
    "\n",
    "            if self.negatives_from_everywhere:\n",
    "                negs, _ = self.sample_negatives(\n",
    "                    unmasked_features,\n",
    "                    y.size(1),\n",
    "                    padding_count=padding_count,\n",
    "                )\n",
    "                negs = self.project_q(negs)\n",
    "            else:\n",
    "                negs, _ = self.sample_negatives(\n",
    "                    y,\n",
    "                    y.size(1),\n",
    "                    padding_count=padding_count,\n",
    "                )\n",
    "\n",
    "        if not is_xla_tensor(x):\n",
    "            # tpu-comment: reducing the size in a dynamic way causes\n",
    "            # too many recompilations on xla.\n",
    "            x = x[mask_indices].view(x.size(0), -1, x.size(-1))\n",
    "\n",
    "        if self.target_glu:\n",
    "            y = self.target_glu(y)\n",
    "            negs = self.target_glu(negs)\n",
    "\n",
    "        x = self.final_proj(x)\n",
    "        x = self.compute_preds(x, y, negs)\n",
    "\n",
    "        result = {\n",
    "            \"x\": x,\n",
    "            \"padding_mask\": padding_mask,\n",
    "            \"features_pen\": features_pen,\n",
    "        }\n",
    "\n",
    "        if prob_ppl is not None:\n",
    "            result[\"prob_perplexity\"] = prob_ppl\n",
    "            result[\"code_perplexity\"] = code_ppl\n",
    "            result[\"num_vars\"] = num_vars\n",
    "            result[\"temp\"] = curr_temp\n",
    "\n",
    "        return result\n",
    "\n",
    "    def quantize(self, x):\n",
    "        assert self.quantizer is not None\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.quantizer.forward_idx(x)\n",
    "\n",
    "    def extract_features(self, source, padding_mask, mask=False, layer=None):\n",
    "        res = self.forward(\n",
    "            source, padding_mask, mask=mask, features_only=True, layer=layer\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    def get_logits(self, net_output):\n",
    "        logits = net_output[\"x\"]\n",
    "        logits = logits.transpose(0, 2)\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        return logits\n",
    "\n",
    "    def get_targets(self, sample, net_output, expand_steps=True):\n",
    "        x = net_output[\"x\"]\n",
    "        return x.new_zeros(x.size(1) * x.size(2), dtype=torch.long)\n",
    "\n",
    "    def get_extra_losses(self, net_output):\n",
    "        pen = []\n",
    "\n",
    "        if \"prob_perplexity\" in net_output:\n",
    "            pen.append(\n",
    "                (net_output[\"num_vars\"] - net_output[\"prob_perplexity\"])\n",
    "                / net_output[\"num_vars\"]\n",
    "            )\n",
    "\n",
    "        if \"features_pen\" in net_output:\n",
    "            pen.append(net_output[\"features_pen\"])\n",
    "\n",
    "        return pen\n",
    "\n",
    "    def remove_pretraining_modules(self):\n",
    "        self.quantizer = None\n",
    "        self.project_q = None\n",
    "        self.target_glu = None\n",
    "        self.final_proj = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7dc86454-ec24-49ad-aa27-4b97fa62af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Config(_name=None, conv_layer_setting=ConvFeatureExtractionModelConfig(_name=None, extractor_mode='default', conv_feature_layers='[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', conv_bias=False, drop_out=0.5), encoder_setting=TransformerEncoderConfig(_name=None, layer_setting=TransformerSentenceEncoderLayerConfig(_name=None, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, activation_fn='gelu', layer_norm_first=False), encoder_layers=12, conv_pos=128, conv_pos_groups=16, encoder_layerdrop=0.0), dropout_input=0.0, dropout_features=0.0, final_dim=0, logit_temp=0.1, quantize_targets=False, quantize_input=False, same_quantizer=False, target_glu=False, feature_grad_mult=1.0, quantizer_depth=1, quantizer_factor=3, latent_vars=320, latent_groups=2, latent_dim=0, mask_length=10, mask_prob=0.65, mask_selection='static', mask_other=0, no_mask_overlap=False, mask_min_space=1, mask_channel_length=10, mask_channel_prob=0.0, mask_channel_before=False, mask_channel_selection='static', mask_channel_other=0, no_mask_channel_overlap=False, mask_channel_min_space=1, num_negatives=100, negatives_from_everywhere=False, cross_sample_negatives=0, codebook_negatives=0, latent_temp=(2, 0.5, 0.999995))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Wav2Vec2Config()\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c357e19-eded-4de2-bc6e-811c5ea67c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): ConvFeatureExtractionModel(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        (3): GELU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (dropout_input): Dropout(p=0.0, inplace=False)\n",
       "  (dropout_features): Dropout(p=0.0, inplace=False)\n",
       "  (project_q): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (pos_conv): Sequential(\n",
       "      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (1): SamePad()\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "  (final_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2Model(config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "200c9fa8-5967-44e2-9fcd-504fbec98da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Wav2Vec2Model                                           --                        --\n",
       "ConvFeatureExtractionModel: 1                         --                        --\n",
       "    ModuleList: 2-1                                  --                        --\n",
       "TransformerEncoder: 1                                 --                        --\n",
       "    ModuleList: 2-2                                  --                        --\n",
       "ConvFeatureExtractionModel: 1-1                       [1, 512, 27]              --\n",
       "    ModuleList: 2-1                                  --                        --\n",
       "        Sequential: 3-1                             [1, 512, 1799]            6,144\n",
       "        Sequential: 3-2                             [1, 512, 899]             786,432\n",
       "        Sequential: 3-3                             [1, 512, 449]             786,432\n",
       "        Sequential: 3-4                             [1, 512, 224]             786,432\n",
       "        Sequential: 3-5                             [1, 512, 111]             786,432\n",
       "        Sequential: 3-6                             [1, 512, 55]              524,288\n",
       "        Sequential: 3-7                             [1, 512, 27]              524,288\n",
       "FusedLayerNorm: 1-2                                   [1, 27, 512]              1,024\n",
       "Linear: 1-3                                           [1, 27, 768]              393,984\n",
       "Dropout: 1-4                                          [1, 27, 768]              --\n",
       "Dropout: 1-5                                          [1, 27, 512]              --\n",
       "TransformerEncoder: 1-6                               [1, 27, 768]              --\n",
       "    Sequential: 2-3                                  [1, 768, 27]              --\n",
       "        Conv1d: 3-8                                 [1, 768, 28]              4,719,488\n",
       "        SamePad: 3-9                                [1, 768, 27]              --\n",
       "        GELU: 3-10                                  [1, 768, 27]              --\n",
       "    FusedLayerNorm: 2-4                              [1, 27, 768]              1,536\n",
       "    ModuleList: 2-2                                  --                        --\n",
       "        TransformerSentenceEncoderLayer: 3-11       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-12       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-13       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-14       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-15       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-16       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-17       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-18       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-19       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-20       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-21       [27, 1, 768]              7,087,872\n",
       "        TransformerSentenceEncoderLayer: 3-22       [27, 1, 768]              7,087,872\n",
       "Linear: 1-7                                           [1, 15, 768]              393,984\n",
       "Linear: 1-8                                           [1, 15, 768]              590,592\n",
       "=========================================================================================================\n",
       "Total params: 67,007,104\n",
       "Trainable params: 67,007,104\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.53\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 36.70\n",
       "Params size (MB): 268.03\n",
       "Estimated Total Size (MB): 304.76\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, (1,9000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e126c27-58a6-4d71-9a4a-5ef06c233ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
